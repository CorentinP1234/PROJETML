{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d099b92b-f01e-4232-8835-e318f57769ee",
   "metadata": {},
   "source": [
    "# Modéles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738d6ce-1f8b-4544-8759-2dbfd669f918",
   "metadata": {},
   "source": [
    "## Régression Linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0a118-62a8-415f-9b3b-10f5118ba4a3",
   "metadata": {},
   "source": [
    "La régression linéaire est une méthode d'apprentissage supervisé utilisée pour modéliser la relation entre une variable dépendante (cible) et une ou plusieurs variables indépendantes (prédicteurs). Le modèle linéaire est représenté par l'équation suivante :\n",
    "\n",
    "$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_N x_N + \\epsilon$\n",
    "\n",
    "Ici, $y$ est la variable dépendante, $x_i$ sont les variables indépendantes, $\\theta_i$ sont les coefficients à estimer et $\\epsilon$ est l'erreur résiduelle.\n",
    "\n",
    "L'objectif de la régression linéaire est de minimiser le coût, généralement la somme des erreurs quadratiques (SSE). Pour ce faire, on utilise une méthode d'optimisation telle que la descente de gradient, qui ajuste les coefficients $\\theta_i$ pour réduire le coût. La mise à jour des coefficients se fait en utilisant la formule suivante :\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} coût(\\theta_0, \\theta_1, ..., \\theta_N)$\n",
    "\n",
    "Où $\\alpha$ est le taux d'apprentissage, un paramètre qui contrôle la vitesse à laquelle les coefficients sont ajustés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6b1ee-59c1-4b27-858c-be4ad92e2cb7",
   "metadata": {},
   "source": [
    "## Régrésion linéaire régularisée\n",
    "\n",
    "Le modèle de régression linéaire est très utile dans beaucoup de cas, mais elle possède des limites.\n",
    "\n",
    "<img src=\"images/reg_intro.png\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "Comme nous l'indique cette image, le problème de la régréssion linéaire est l'overfiting : c'est lorsque le modèle s'ajuste trop  aux données d'entrainement ce qui as pour conséquences, qu'il n'arrive pas à généraliser pour prédir de nouveaux exemples. Pour palier à ce problème, nous utilisons la régression linéaire régularisé qui fonctionent de la même manière que la régression linéaire classique en ajoutant une \"pénalité\" afin de réajuster la droite et ne pas prendre trop de valeurs lors de l'apprentissage.\n",
    "\n",
    "Les deux modèles que nous allons utiliser et détailler sont Ridge et Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977746-11e9-4b67-9511-6497be004dea",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a6545-1faa-4c71-af47-14201636b0b5",
   "metadata": {},
   "source": [
    "La régression linéaire avec pénalité L2, également connue sous le nom de Ridge Regression, est une autre extension de la régression linéaire classique. La principale différence réside dans l'introduction d'un terme de pénalité L2 dans la fonction coût, qui aide à réduire la complexité du modèle en limitant la magnitude des coefficients de régression.\n",
    "\n",
    "La mise à jour des coefficients en utilisant la pénalité L2 est représentée par la formule suivante :\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} \\left( coût(\\theta_0, \\theta_1, ..., \\theta_N) + \\lambda \\sum_{j=1}^N \\theta_j^2 \\right)$\n",
    "\n",
    "Ici, $\\lambda$ est un paramètre de régularisation qui contrôle l'importance relative de la pénalité L2 dans la fonction coût. Lorsque $\\lambda = 0$, le modèle de régression avec pénalité L2 revient au modèle de régression linéaire classique.\n",
    "\n",
    "L'avantage de la pénalité L2 est qu'elle encourage la régularisation des coefficients en les rétrécissant, sans pour autant les pousser vers zéro. Cela conduit à un modèle plus robuste face à la multicollinéarité et réduit le risque de d'overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97ba7b-76b4-4ced-856d-3bad7a05424c",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d61bdb-fb58-4831-b4f6-a564db09c229",
   "metadata": {},
   "source": [
    "La régression linéaire avec pénalité L1, également connue sous le nom de Lasso (Least Absolute Shrinkage and Selection Operator), est une extension de la régression linéaire classique. \n",
    "<br>\n",
    "Le modèle de régression linéaire Lasso (Least Absolute Shrinkage and Selection Operator) est une extension de la régression linéaire classique avec une pénalité L1.\n",
    "La principale différence réside dans l'introduction d'un terme de pénalité dans la fonction coût, qui aide à réduire la complexité du modèle en favorisant des coefficients de régression plus petits.\n",
    "\n",
    "La mise à jour des coefficients en utilisant la pénalité L1 est représentée par la formule suivante :\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} \\left( coût(\\theta_0, \\theta_1, ..., \\theta_N) + \\lambda \\sum_{j=1}^N |\\theta_j| \\right)$\n",
    "\n",
    "Ici, $\\lambda$ est un paramètre de régularisation qui contrôle l'importance relative de la pénalité L1 dans la fonction coût. Lorsque $\\lambda = 0$, le modèle de régression avec pénalité L1 revient au modèle de régression linéaire classique.\n",
    "\n",
    "L'avantage de la pénalité L1 est qu'elle pousse certains coefficients de régression vers zéro. Cela conduit à un modèle plus simple et interprétable, avec un risque d'overfitting réduit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf48d5e-4e89-4043-8f47-99add1a9fab6",
   "metadata": {},
   "source": [
    "## K-Neighbors Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a25937-3f87-42f2-82c2-d700255c8806",
   "metadata": {},
   "source": [
    "La régression K-Neighbors (KNN) est une méthode d'apprentissage supervisé basée sur la proximité des points dans l'espace des caractéristiques. Elle est utilisée pour estimer la valeur d'une variable dépendante en fonction de la moyenne des valeurs de ses k voisins les plus proches. La formule pour la prédiction de KNN est la suivante :\n",
    "\n",
    "$ \\hat{y} = \\frac{1}{k} \\sum_{i \\in V_k(x)} y_i$\n",
    "\n",
    "Ici, $V_k(x)$ représente l'ensemble des $k$ voisins les plus proches du point $x$, $y_i$ sont les valeurs de la variable dépendante pour les points voisins, et $\\hat{y}$ est la prédiction pour le point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674bec6-98e3-42f5-8cca-f8e6e2ebcce2",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30df31f-0214-4824-9ded-49fb3a0467f4",
   "metadata": {},
   "source": [
    "Un arbre de décision est un algorithme d'apprentissage automatique supervisé utilisé pour résoudre des problèmes de classification et de régression. L'objectif de cet algorithme est de créer un arbre de décision qui peut être utilisé pour prendre des décisions en fonction des caractéristiques d'un ensemble de données.\n",
    "\n",
    "Le processus de création d'un arbre de décision implique de diviser l'ensemble de données en plusieurs sous-ensembles en fonction des valeurs de caractéristiques. Cette division est effectuée de manière à maximiser la différence entre les classes de sortie dans chaque sous-ensemble. Ce processus est répété de manière récursive pour chaque sous-ensemble jusqu'à ce qu'un critère d'arrêt soit atteint.\n",
    "\n",
    "   <img src=\"images/decision_tree.png\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "Dans cet exemple nous voyons que chaque noeud de l'arbre correspond à une condition sur une variable. Et les feuilles correspondent à des valeurs que vont prendre différentes valeurs possibles pour la variable dépendante.\n",
    "\n",
    "Les avantages de l'utilisation de l'arbre de décision comprennent la facilité d'interprétation et la capacité à gérer des données manquantes. Cependant, l'arbre de décision peut souffrir de surapprentissage et peut ne pas être aussi précis que d'autres algorithmes d'apprentissage automatique dans certaines situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a868b-31eb-4604-9739-98441d8c9fdd",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb741f-31c9-41e4-81e7-17f8a7434cbb",
   "metadata": {},
   "source": [
    "La régression Random Forest est une méthode d'apprentissage ensembliste basée sur l'agrégation de nombreux arbres de décision. Elle tire parti de la \"Wisdom of Crowds\" (sagesse des foules) en combinant les prédictions de plusieurs arbres pour obtenir une meilleure performance que les prédictions individuelles. La méthode d'agrégation utilisée est appelée \"Bagging\" (Bootstrap Aggregating), qui consiste à créer des sous-ensembles de données avec remise et à entraîner un arbre de décision pour chaque sous-ensemble. Les prédictions finales sont obtenues en moyennant les prédictions de tous les arbres. La Random Forest est robuste face au overfitting et offre souvent une meilleure performance que les modèles de régression linéaire, en particulier lorsque les relations entre les variables sont non linéaires. <br>\n",
    "<img src=\"images/random_forest.png\" alt=\"Alt text\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847927d3-c3f9-4295-a426-2a6add94bd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b74cd9d-84cb-41d5-b8c6-f9d7fc9de126",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Métriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79f637-5789-4122-a3ce-6c3308e2097a",
   "metadata": {},
   "source": [
    "Les métriques sont des indicateurs clés pour évaluer et comparer les performances des modèles de machine learning, en particulier dans le contexte de la régression et la prédiction de valeurs numériques. Parmi les métriques couramment utilisées, on retrouve le RMSE, R2 et le coefficient de corrélation de Spearman. Chacune de ces métriques permet de quantifier la performance d'un modèle de manière différente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390c025-1dcd-4caf-8e4d-53c46352a211",
   "metadata": {},
   "source": [
    "## RMSE : L'erreur quadratique moyenne "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03789b4-6c6f-4f4b-a947-70b0c34db4fa",
   "metadata": {},
   "source": [
    "Le RMSE est une mesure de l'erreur moyenne quadratique entre les valeurs prédites et les valeurs réelles. Il s'agit de la racine carrée de la moyenne des erreurs quadratiques, ce qui permet de quantifier la différence entre les valeurs prédites et observées. Un RMSE plus faible indique une meilleure précision du modèle. Toutefois, le RMSE étant sensible aux valeurs extrêmes, il peut parfois donner une image trompeuse de la performance d'un modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf60d2e-105c-45da-9aab-b9b18a5499c9",
   "metadata": {},
   "source": [
    "## R2 : Coefficient de détermination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a92de0-8c36-4c42-bfef-eda0bff9b667",
   "metadata": {},
   "source": [
    "Le R2 est une mesure statistique qui indique la proportion de la variance des données expliquée par le modèle de régression. Il varie entre 0 et 1, où 1 indique que le modèle explique parfaitement la variance des données et 0 signifie qu'il n'explique pas du tout la variance. Un R2 élevé (proche de 1) indique que le modèle s'ajuste bien aux données, tandis qu'un R2 faible (proche 0) suggère un ajustement médiocre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f541c-e532-46dc-ae0c-03d458ab8d28",
   "metadata": {},
   "source": [
    "## Coefficient de corrélation de Spearman "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b593e-fd83-43b1-ba90-d246f490bc03",
   "metadata": {},
   "source": [
    "Le coefficient de corrélation de Spearman mesure la force et la direction de la relation monotone entre deux variables. Il varie entre -1 et 1. Une valeur proche de 1 indique une relation monotone positive forte entre les variables, tandis qu'une valeur proche de -1 indique une relation monotone négative forte. Une valeur proche de 0 suggère l'absence de corrélation monotone. Contrairement à la corrélation de Pearson, le coefficient de Spearman est moins sensible aux valeurs extrêmes et peut être utilisé pour évaluer la relation entre des variables non linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb76ed0-9328-4197-9857-429ea9aebd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
