{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd21aa03-0baa-4a2c-9514-c1339bbbe016",
   "metadata": {},
   "source": [
    "# 4 Modelisation des donn ´ ees ´\n",
    "Diff´erents algorithmes de Machine Learning seront utilis´es pour entraˆıner des mod`eles de pr´ediction a partir `\n",
    "des donn´ees. Nous proposons d’impl´ementer les six mod`eles de r´egression :\n",
    "1. R´egression lin´eaire simple\n",
    "2. R´egression lin´eaire r´egularis´ee (r´egression RIDGE / r´egression LASSO) : voir lien suivant R´egression\n",
    "Ridge, Lasso et nouvel estimateur\n",
    "3. M´ethode des k plus proches voisins pour la r´egression (K-NN, k-Nearest Neighbors regressor) http://www.xavierdupre.fr/app/papierstat/helpsphinx/notebooks/2020-02-07_sklapi.html\n",
    "4. Arbres de d´ecision pour la r´egression (Decision tree regressor)\n",
    "5. En bonus, les Forˆets al´eatoires (Random Forest regressor)\n",
    "Vous devez comprendre les deux variantes de r´egression lin´eaire r´egularis´ee et les d´ecrire bri`evement dans\n",
    "votre rapport. Pareil pour la m´ethode bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738d6ce-1f8b-4544-8759-2dbfd669f918",
   "metadata": {},
   "source": [
    "## Régression Linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0a118-62a8-415f-9b3b-10f5118ba4a3",
   "metadata": {},
   "source": [
    "La régression linéaire est une méthode d'apprentissage supervisé utilisée pour modéliser la relation entre une variable dépendante (cible) et une ou plusieurs variables indépendantes (prédicteurs). Le modèle linéaire est représenté par l'équation suivante :\n",
    "\n",
    "$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_N x_N + \\epsilon$\n",
    "\n",
    "Ici, $y$ est la variable dépendante, $x_i$ sont les variables indépendantes, $\\theta_i$ sont les coefficients à estimer et $\\epsilon$ est l'erreur résiduelle.\n",
    "\n",
    "L'objectif de la régression linéaire est de minimiser le coût, généralement la somme des erreurs quadratiques (SSE). Pour ce faire, on utilise une méthode d'optimisation telle que la descente de gradient, qui ajuste les coefficients $\\theta_i$ pour réduire le coût. La mise à jour des coefficients se fait en utilisant la formule suivante :\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} coût(\\theta_0, \\theta_1, ..., \\theta_N)$\n",
    "\n",
    "Où $\\alpha$ est le taux d'apprentissage, un paramètre qui contrôle la vitesse à laquelle les coefficients sont ajustés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977746-11e9-4b67-9511-6497be004dea",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a6545-1faa-4c71-af47-14201636b0b5",
   "metadata": {},
   "source": [
    "La régression linéaire avec pénalité L2, également connue sous le nom de Ridge Regression, est une autre extension de la régression linéaire classique. La principale différence réside dans l'introduction d'un terme de pénalité L2 dans la fonction coût, qui aide à réduire la complexité du modèle en limitant la magnitude des coefficients de régression.\n",
    "\n",
    "La mise à jour des coefficients en utilisant la pénalité L2 est représentée par la formule suivante :\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} \\left( coût(\\theta_0, \\theta_1, ..., \\theta_N) + \\lambda \\sum_{j=1}^N \\theta_j^2 \\right)$\n",
    "\n",
    "Ici, $\\lambda$ est un paramètre de régularisation qui contrôle l'importance relative de la pénalité L2 dans la fonction coût. Lorsque $\\lambda = 0$, le modèle de régression avec pénalité L2 revient au modèle de régression linéaire classique.\n",
    "\n",
    "L'avantage de la pénalité L2 est qu'elle encourage la régularisation des coefficients en les rétrécissant, sans pour autant les pousser vers zéro. Cela conduit à un modèle plus robuste face à la multicollinéarité et réduit le risque de d'overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97ba7b-76b4-4ced-856d-3bad7a05424c",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d61bdb-fb58-4831-b4f6-a564db09c229",
   "metadata": {},
   "source": [
    "La régression linéaire avec pénalité L1, également connue sous le nom de Lasso (Least Absolute Shrinkage and Selection Operator), est une extension de la régression linéaire classique. \n",
    "<br>\n",
    "Le modèle de régression linéaire Lasso (Least Absolute Shrinkage and Selection Operator) est une extension de la régression linéaire classique avec une pénalité L1.\n",
    "La principale différence réside dans l'introduction d'un terme de pénalité dans la fonction coût, qui aide à réduire la complexité du modèle en favorisant des coefficients de régression plus petits.\n",
    "\n",
    "La mise à jour des coefficients en utilisant la pénalité L1 est représentée par la formule suivante :\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} \\left( coût(\\theta_0, \\theta_1, ..., \\theta_N) + \\lambda \\sum_{j=1}^N |\\theta_j| \\right)$\n",
    "\n",
    "Ici, $\\lambda$ est un paramètre de régularisation qui contrôle l'importance relative de la pénalité L1 dans la fonction coût. Lorsque $\\lambda = 0$, le modèle de régression avec pénalité L1 revient au modèle de régression linéaire classique.\n",
    "\n",
    "L'avantage de la pénalité L1 est qu'elle pousse certains coefficients de régression vers zéro. Cela conduit à un modèle plus simple et interprétable, avec un risque d'overfitting réduit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf48d5e-4e89-4043-8f47-99add1a9fab6",
   "metadata": {},
   "source": [
    "## K-Neighbors Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a25937-3f87-42f2-82c2-d700255c8806",
   "metadata": {},
   "source": [
    "La régression K-Neighbors (KNN) est une méthode d'apprentissage supervisé basée sur la proximité des points dans l'espace des caractéristiques. Elle est utilisée pour estimer la valeur d'une variable dépendante en fonction de la moyenne des valeurs de ses k voisins les plus proches. La formule pour la prédiction de KNN est la suivante :\n",
    "\n",
    "$ \\hat{y} = \\frac{1}{k} \\sum_{i \\in V_k(x)} y_i$\n",
    "\n",
    "Ici, $V_k(x)$ représente l'ensemble des $k$ voisins les plus proches du point $x$, $y_i$ sont les valeurs de la variable dépendante pour les points voisins, et $\\hat{y}$ est la prédiction pour le point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674bec6-98e3-42f5-8cca-f8e6e2ebcce2",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a868b-31eb-4604-9739-98441d8c9fdd",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb741f-31c9-41e4-81e7-17f8a7434cbb",
   "metadata": {},
   "source": [
    "La régression Random Forest est une méthode d'apprentissage ensembliste basée sur l'agrégation de nombreux arbres de décision. Elle tire parti de la \"Wisdom of Crowds\" (sagesse des foules) en combinant les prédictions de plusieurs arbres pour obtenir une meilleure performance que les prédictions individuelles. La méthode d'agrégation utilisée est appelée \"Bagging\" (Bootstrap Aggregating), qui consiste à créer des sous-ensembles de données avec remise et à entraîner un arbre de décision pour chaque sous-ensemble. Les prédictions finales sont obtenues en moyennant les prédictions de tous les arbres. La Random Forest est robuste face au overfitting et offre souvent une meilleure performance que les modèles de régression linéaire, en particulier lorsque les relations entre les variables sont non linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47b7d8-6d38-4ecf-967c-4eab40250d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
